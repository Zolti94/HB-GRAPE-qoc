{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Optimizer Comparison**\n",
        "\n",
        "Goal: compare CRAB optimizers on a shared configuration, report convergence/runtime, and evaluate robustness against detuning and pulse-area variations.\n",
        "Quickstart: edit the Parameter Panel, run all cells, then review the Results Summary.\n",
        "\n",
        "Sections:\n",
        "- [Parameter Panel](#Parameter-Panel)\n",
        "- [Run Batch](#Run-Batch)\n",
        "- [Convergence Overlays](#Convergence-Overlays)\n",
        "- [Runtime Table](#Runtime-Table)\n",
        "- [Apples-to-Apples Evaluation](#Apples-to-Apples-Evaluation)\n",
        "- [Robustness Heatmaps](#Robustness-Heatmaps)\n",
        "- [Results Summary](#Results-Summary)\n",
        "- [Troubleshooting](#Troubleshooting)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports\n",
        "from pathlib import Path\n",
        "import json\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import LogNorm\n",
        "\n",
        "from src import (\n",
        "    ExperimentConfig,\n",
        "    override_from_dict,\n",
        "    run_experiment,\n",
        "    plot_cost_history,\n",
        "    plot_pulses,\n",
        "    plot_summary,\n",
        "    plot_penalties_history,\n",
        "    plot_robustness_heatmap,\n",
        "    Result,\n",
        "    accumulate_cost_and_grads,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Parameter Panel\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "parameters"
        ]
      },
      "outputs": [],
      "source": [
        "# @title Parameter Panel\n",
        "BASELINE_ARRAYS = Path('data/baselines/_baseline_crab/arrays.npz')\n",
        "with np.load(BASELINE_ARRAYS) as baseline_npz:\n",
        "    arrays = {key: np.array(baseline_npz[key]) for key in baseline_npz.files}\n",
        "\n",
        "t_us = arrays['t'].astype(float)\n",
        "T_US = float(arrays['T'])\n",
        "M = int(arrays['Nt'])\n",
        "delta0 = arrays['Delta0'].astype(float)\n",
        "delta_max_rad_per_us = float(np.abs(delta0).max())\n",
        "delta_max_MHz = delta_max_rad_per_us / (2 * np.pi)\n",
        "default_K = int(arrays['CRAB_BASIS_OMEGA'].shape[1])\n",
        "default_omegas = (np.pi / T_US * np.arange(1, default_K + 1)).tolist()\n",
        "\n",
        "user_params = {\n",
        "    'run_name_prefix': 'comparison',\n",
        "    'output_dir': 'results/optimizer_comparison',\n",
        "    'methods': ['adam', 'const', 'linesearch'],\n",
        "    'method_settings': {\n",
        "        'adam': {'max_iters': 300, 'learning_rate': 0.05, 'beta1': 0.9, 'beta2': 0.999, 'epsilon': 1e-8, 'grad_clip': None},\n",
        "        'const': {'max_iters': 400, 'learning_rate': 0.02, 'grad_clip': None},\n",
        "        'linesearch': {'max_iters': 300, 'alpha0': 0.2, 'ls_beta': 0.5, 'ls_sigma': 1e-4, 'ls_max_backtracks': 12, 'grad_clip': None},\n",
        "    },\n",
        "    'penalties': {'w_power': 0.0, 'w_neg': 0.0, 'neg_kappa': 10.0},\n",
        "    'crab': {\n",
        "        'K': default_K,\n",
        "        'omegas_rad_per_us': default_omegas,\n",
        "        'phases': [0.0] * default_K,\n",
        "        'use_delta': True,\n",
        "    },\n",
        "    'path_objective': {'method': 'adam', 'max_iters': 300},\n",
        "    'ensemble_objective': {'method': 'adam', 'max_iters': 300},\n",
        "}\n",
        "\n",
        "BASE_OVERRIDES = {\n",
        "    'baseline': {'name': 'default'},\n",
        "    'run_name': user_params['run_name_prefix'],\n",
        "    'artifacts_root': str(Path(user_params['output_dir']).resolve()),\n",
        "    'optimizer_options': {\n",
        "        'method': 'adam',\n",
        "        'max_iters': user_params['method_settings']['adam']['max_iters'],\n",
        "        'learning_rate': user_params['method_settings']['adam']['learning_rate'],\n",
        "        'beta1': user_params['method_settings']['adam']['beta1'],\n",
        "        'beta2': user_params['method_settings']['adam']['beta2'],\n",
        "        'epsilon': user_params['method_settings']['adam']['epsilon'],\n",
        "        'grad_clip': user_params['method_settings']['adam']['grad_clip'],\n",
        "        'optimize_delta': bool(user_params['crab']['use_delta']),\n",
        "        'K': int(user_params['crab']['K']),\n",
        "        'omegas_rad_per_us': user_params['crab']['omegas_rad_per_us'],\n",
        "        'phases': user_params['crab']['phases'],\n",
        "    },\n",
        "    'penalties': {\n",
        "        'power_weight': float(user_params['penalties']['w_power']),\n",
        "        'neg_weight': float(user_params['penalties']['w_neg']),\n",
        "        'neg_kappa': float(user_params['penalties']['neg_kappa']),\n",
        "    },\n",
        "    'metadata': {\n",
        "        'objective': 'terminal',\n",
        "        'path_params': {'reference': 'adiabatic_ground_state'},\n",
        "        'ensemble_params': {\n",
        "            'beta_min': 0.9,\n",
        "            'beta_max': 1.1,\n",
        "            'num_beta': 5,\n",
        "            'detuning_MHz_min': -delta_max_MHz,\n",
        "            'detuning_MHz_max': delta_max_MHz,\n",
        "            'num_detuning': 5,\n",
        "        },\n",
        "    },\n",
        "}\n",
        "\n",
        "print('Methods:', user_params['methods'])\n",
        "print('Path objective settings:', user_params['path_objective'])\n",
        "print('Ensemble objective settings:', user_params['ensemble_objective'])\n",
        "print('Penalties:', user_params['penalties'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run Batch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title Run Batch\n",
        "from copy import deepcopy\n",
        "\n",
        "OUTPUT_DIR = Path(user_params['output_dir']).resolve()\n",
        "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "FIGURES_DIR = OUTPUT_DIR / 'figures'\n",
        "FIGURES_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "results_store = {}\n",
        "terminal_runs = {}\n",
        "\n",
        "base_config = override_from_dict(ExperimentConfig(), BASE_OVERRIDES)\n",
        "problem, _, _ = load_crab_problem(base_config)\n",
        "\n",
        "omega_baseline = problem.omega_base\n",
        "if problem.delta_base is None:\n",
        "    delta_baseline = np.zeros_like(omega_baseline)\n",
        "else:\n",
        "    delta_baseline = problem.delta_base\n",
        "\n",
        "baseline_cost, _ = accumulate_cost_and_grads(\n",
        "    omega_baseline,\n",
        "    delta_baseline,\n",
        "    problem.dt_us,\n",
        "    psi0=problem.psi0,\n",
        "    psi_target=problem.psi_target,\n",
        "    w_power=0.0,\n",
        "    w_neg=0.0,\n",
        ")\n",
        "results_store['baseline'] = {\n",
        "    'objective': 'baseline',\n",
        "    'metrics': {\n",
        "        'terminal': float(baseline_cost.get('terminal', 0.0)),\n",
        "        'total': float(baseline_cost.get('terminal', 0.0)),\n",
        "        'terminal_eval': float(baseline_cost.get('terminal', 0.0)),\n",
        "    },\n",
        "    'pulses': {'omega': omega_baseline, 'delta': delta_baseline},\n",
        "    'artifacts_dir': None,\n",
        "}\n",
        "\n",
        "\n",
        "def build_overrides(objective: str, method: str, method_opts: dict, suffix: str) -> dict:\n",
        "    overrides = json.loads(json.dumps(BASE_OVERRIDES))\n",
        "    overrides['run_name'] = f\"{user_params['run_name_prefix']}-{objective}-{method}-{suffix}\"\n",
        "    overrides['optimizer_options'].update(method_opts)\n",
        "    overrides['optimizer_options']['method'] = method\n",
        "    overrides['metadata']['objective'] = objective\n",
        "    return overrides\n",
        "\n",
        "# Terminal runs for each method\n",
        "for method in user_params['methods']:\n",
        "    method_opts = deepcopy(user_params['method_settings'][method])\n",
        "    overrides = build_overrides('terminal', method, method_opts, 'opt')\n",
        "    cfg = override_from_dict(ExperimentConfig(), overrides)\n",
        "    result = run_experiment(cfg, method=method)\n",
        "    terminal_runs[method] = result\n",
        "    results_store[f\"terminal_{method}\"] = {\n",
        "        'objective': 'terminal',\n",
        "        'result': result,\n",
        "    }\n",
        "\n",
        "# Path optimisation (single method)\n",
        "path_method = user_params['path_objective'].get('method', 'adam')\n",
        "path_opts = deepcopy(user_params['method_settings'][path_method])\n",
        "path_opts.update({k: v for k, v in user_params['path_objective'].items() if k != 'method'})\n",
        "path_overrides = build_overrides('path', path_method, path_opts, 'opt')\n",
        "cfg_path = override_from_dict(ExperimentConfig(), path_overrides)\n",
        "path_result = run_experiment(cfg_path, method=path_method)\n",
        "results_store['path'] = {'objective': 'path', 'result': path_result}\n",
        "\n",
        "# Ensemble optimisation (single method)\n",
        "ensemble_method = user_params['ensemble_objective'].get('method', 'adam')\n",
        "ensemble_opts = deepcopy(user_params['method_settings'][ensemble_method])\n",
        "ensemble_opts.update({k: v for k, v in user_params['ensemble_objective'].items() if k != 'method'})\n",
        "ensemble_overrides = build_overrides('ensemble', ensemble_method, ensemble_opts, 'opt')\n",
        "cfg_ens = override_from_dict(ExperimentConfig(), ensemble_overrides)\n",
        "ensemble_result = run_experiment(cfg_ens, method=ensemble_method)\n",
        "results_store['ensemble'] = {'objective': 'ensemble', 'result': ensemble_result}\n",
        "\n",
        "print('Completed runs:')\n",
        "for key, entry in results_store.items():\n",
        "    if key == 'baseline':\n",
        "        print('  baseline (no artifacts)')\n",
        "    else:\n",
        "        res = entry['result']\n",
        "        print(f\"  {key}: status={res.optimizer_state.get('status')} -> {res.artifacts_dir}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Convergence Overlays\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title Convergence Overlays\n",
        "conv_paths = []\n",
        "plt.rcParams.update({'font.size': 11})\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(7, 5))\n",
        "for method, result in terminal_runs.items():\n",
        "    total = result.history.get('total')\n",
        "    if total is None or len(total) == 0:\n",
        "        continue\n",
        "    iterations = result.history.get('iter')\n",
        "    if iterations is None or len(iterations) != len(total):\n",
        "        iterations = np.arange(1, len(total) + 1)\n",
        "    ax.semilogy(iterations, total, label=method)\n",
        "ax.set_xlabel('Iteration')\n",
        "ax.set_ylabel('Total cost')\n",
        "ax.set_title('Cost vs iteration (terminal objective)')\n",
        "ax.grid(True, which='both', ls=':', alpha=0.6)\n",
        "ax.legend()\n",
        "conv_file = FIGURES_DIR / 'convergence_cost_vs_iteration.png'\n",
        "fig.savefig(conv_file, dpi=300, bbox_inches='tight')\n",
        "plt.close(fig)\n",
        "conv_paths.append(conv_file)\n",
        "\n",
        "calls_file = None\n",
        "if any('calls_per_iter' in res.history for res in terminal_runs.values()):\n",
        "    fig, ax = plt.subplots(figsize=(7, 5))\n",
        "    for method, result in terminal_runs.items():\n",
        "        total = result.history.get('total')\n",
        "        calls = result.history.get('calls_per_iter')\n",
        "        if total is None or calls is None or len(total) == 0:\n",
        "            continue\n",
        "        calls = np.asarray(calls)\n",
        "        if calls.size != len(total):\n",
        "            calls = np.arange(1, len(total) + 1)\n",
        "        ax.semilogy(calls, total, label=method)\n",
        "    ax.set_xlabel('Oracle calls')\n",
        "    ax.set_ylabel('Total cost')\n",
        "    ax.set_title('Cost vs oracle calls (terminal objective)')\n",
        "    ax.grid(True, which='both', ls=':', alpha=0.6)\n",
        "    ax.legend()\n",
        "    calls_file = FIGURES_DIR / 'convergence_cost_vs_calls.png'\n",
        "    fig.savefig(calls_file, dpi=300, bbox_inches='tight')\n",
        "    plt.close(fig)\n",
        "    conv_paths.append(calls_file)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(7, 5))\n",
        "for method, result in terminal_runs.items():\n",
        "    grad = result.history.get('grad_norm')\n",
        "    if grad is None or len(grad) == 0:\n",
        "        continue\n",
        "    iterations = result.history.get('iter')\n",
        "    if iterations is None or len(iterations) != len(grad):\n",
        "        iterations = np.arange(1, len(grad) + 1)\n",
        "    ax.semilogy(iterations, grad, label=method)\n",
        "ax.set_xlabel('Iteration')\n",
        "ax.set_ylabel('Gradient norm')\n",
        "ax.set_title('Gradient norm vs iteration (terminal objective)')\n",
        "ax.grid(True, which='both', ls=':', alpha=0.6)\n",
        "ax.legend()\n",
        "grad_file = FIGURES_DIR / 'convergence_grad_vs_iteration.png'\n",
        "fig.savefig(grad_file, dpi=300, bbox_inches='tight')\n",
        "plt.close(fig)\n",
        "conv_paths.append(grad_file)\n",
        "\n",
        "print('Saved convergence overlays:')\n",
        "for path in conv_paths:\n",
        "    print(' ', path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Runtime Table\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title Runtime Table\n",
        "print(f\"{'method':>12}  {'iters':>6}  {'runtime_s':>12}  {'total_cost':>12}\")\n",
        "for method, result in terminal_runs.items():\n",
        "    iterations = len(result.history.get('iter', []))\n",
        "    runtime_s = float(result.final_metrics.get('runtime_s', float('nan')))\n",
        "    total_cost = float(result.final_metrics.get('total', float('nan')))\n",
        "    print(f\"{method:>12}  {iterations:6d}  {runtime_s:12.3f}  {total_cost:12.6f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Apples-to-Apples Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title Apples-to-Apples Terminal Evaluation\n",
        "from collections import OrderedDict\n",
        "\n",
        "def terminal_eval_from_result(result: Result) -> float:\n",
        "    omega = np.asarray(result.pulses['omega'], dtype=float)\n",
        "    delta = np.asarray(result.pulses.get('delta'))\n",
        "    if delta is None:\n",
        "        delta = np.asarray(problem.delta_base, dtype=float)\n",
        "    cost_eval, _ = accumulate_cost_and_grads(\n",
        "        omega,\n",
        "        delta,\n",
        "        problem.dt_us,\n",
        "        psi0=problem.psi0,\n",
        "        psi_target=problem.psi_target,\n",
        "        w_power=0.0,\n",
        "        w_neg=0.0,\n",
        "    )\n",
        "    return float(cost_eval.get('terminal', float('nan')))\n",
        "\n",
        "terminal_eval_table = OrderedDict()\n",
        "terminal_eval_table['baseline'] = terminal_eval_from_result(Result(\n",
        "    run_name='baseline',\n",
        "    artifacts_dir=None,\n",
        "    config=base_config,\n",
        "    history={},\n",
        "    final_metrics={},\n",
        "    pulses={'omega': omega_baseline, 'delta': delta_baseline, 't_us': t_us, 'omega_base': omega_baseline, 'delta_base': delta_baseline, 'coeffs': np.zeros_like(omega_baseline)},\n",
        "    optimizer_state={},\n",
        "))\n",
        "\n",
        "for method, result in terminal_runs.items():\n",
        "    terminal_eval_table[f'terminal_{method}'] = terminal_eval_from_result(result)\n",
        "\n",
        "terminal_eval_table['path'] = terminal_eval_from_result(path_result)\n",
        "terminal_eval_table['ensemble'] = terminal_eval_from_result(ensemble_result)\n",
        "\n",
        "print(f\"{'run':>16}  {'terminal_infidelity':>20}\")\n",
        "for label, value in terminal_eval_table.items():\n",
        "    print(f\"{label:>16}  {value:20.6f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Robustness Heatmaps\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title Robustness Heatmaps\n",
        "detuning_MHz_grid = np.linspace(-delta_max_MHz, delta_max_MHz, 51)\n",
        "area_pi_grid = np.linspace(0.1, 8.0, 51)\n",
        "\n",
        "heatmap_inputs = {\n",
        "    'baseline': {'omega': omega_baseline, 'delta': delta_baseline, 'title': 'Baseline'},\n",
        "}\n",
        "\n",
        "best_terminal_label = None\n",
        "best_terminal_value = float('inf')\n",
        "for method, result in terminal_runs.items():\n",
        "    value = terminal_eval_table[f'terminal_{method}']\n",
        "    heatmap_inputs[f'terminal_{method}'] = {\n",
        "        'omega': np.asarray(result.pulses['omega'], dtype=float),\n",
        "        'delta': np.asarray(result.pulses.get('delta', delta_baseline), dtype=float),\n",
        "        'title': f'Terminal ({method})',\n",
        "    }\n",
        "    if value < best_terminal_value:\n",
        "        best_terminal_value = value\n",
        "        best_terminal_label = f'terminal_{method}'\n",
        "\n",
        "heatmap_inputs['path'] = {\n",
        "    'omega': np.asarray(path_result.pulses['omega'], dtype=float),\n",
        "    'delta': np.asarray(path_result.pulses.get('delta', delta_baseline), dtype=float),\n",
        "    'title': 'Path optimised',\n",
        "}\n",
        "heatmap_inputs['ensemble'] = {\n",
        "    'omega': np.asarray(ensemble_result.pulses['omega'], dtype=float),\n",
        "    'delta': np.asarray(ensemble_result.pulses.get('delta', delta_baseline), dtype=float),\n",
        "    'title': 'Ensemble optimised',\n",
        "}\n",
        "\n",
        "panel_labels = [\n",
        "    ('baseline', heatmap_inputs['baseline']),\n",
        "    ('terminal', heatmap_inputs[best_terminal_label]),\n",
        "    ('path', heatmap_inputs['path']),\n",
        "    ('ensemble', heatmap_inputs['ensemble']),\n",
        "]\n",
        "\n",
        "inf_cache = {}\n",
        "finite_values = []\n",
        "for label, info in panel_labels:\n",
        "    _, inf = plot_robustness_heatmap(\n",
        "        {'omega': info['omega'], 'delta': info['delta']},\n",
        "        t_us,\n",
        "        delta_baseline,\n",
        "        detuning_MHz_grid,\n",
        "        area_pi_grid,\n",
        "        label=label,\n",
        "        psi0=problem.psi0,\n",
        "        target=problem.psi_target,\n",
        "        save_dir=FIGURES_DIR,\n",
        "        save=True,\n",
        "    )\n",
        "    inf_cache[label] = inf\n",
        "    finite_vals = inf[np.isfinite(inf) & (inf > 0.0)]\n",
        "    if finite_vals.size:\n",
        "        finite_values.append(finite_vals.min())\n",
        "        finite_values.append(finite_vals.max())\n",
        "\n",
        "if finite_values:\n",
        "    vmin = min(finite_values)\n",
        "    vmax = max(finite_values)\n",
        "else:\n",
        "    vmin, vmax = 1e-6, 1.0\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(10, 8), sharex=True, sharey=True)\n",
        "axes = axes.flatten()\n",
        "for ax, (label, info) in zip(axes, panel_labels):\n",
        "    inf = inf_cache[label]\n",
        "    X, Y = np.meshgrid(detuning_MHz_grid, area_pi_grid)\n",
        "    mesh = ax.pcolormesh(X, Y, inf, shading='auto', cmap='viridis', norm=LogNorm(vmin=max(vmin, 1e-8), vmax=max(vmax, 1e-6)))\n",
        "    ax.set_title(info['title'])\n",
        "    ax.set_xlabel('Detuning offset \u0394\u2080 (MHz)')\n",
        "    ax.set_ylabel('Pulse area (\u03c0 units)')\n",
        "\n",
        "fig.subplots_adjust(right=0.88, hspace=0.25)\n",
        "cbar_ax = fig.add_axes([0.9, 0.15, 0.02, 0.7])\n",
        "fig.colorbar(mesh, cax=cbar_ax).set_label('Terminal infidelity')\n",
        "combined_heatmap = FIGURES_DIR / 'heatmap_terminal_vs_detuning_area_4panel.png'\n",
        "fig.savefig(combined_heatmap, dpi=300, bbox_inches='tight')\n",
        "fig.savefig(combined_heatmap.with_suffix('.svg'), bbox_inches='tight')\n",
        "plt.close(fig)\n",
        "\n",
        "print('Heatmap outputs:')\n",
        "print('  ', combined_heatmap)\n",
        "print('  ', combined_heatmap.with_suffix('.svg'))\n",
        "for label, _ in panel_labels:\n",
        "    print('  ', FIGURES_DIR / f'heatmap_terminal_vs_detuning_area_{label}.png')\n",
        "    print('  ', FIGURES_DIR / f'heatmap_terminal_vs_detuning_area_{label}.svg')\n",
        "    print('  ', FIGURES_DIR / f'robustness_terminal_vs_detuning_area_{label}.npz')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Results Summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title Results Summary\n",
        "from pathlib import Path\n",
        "\n",
        "print(f\"{'label':>16}  {'objective':>10}  {'iters':>6}  {'total_cost':>12}  {'terminal_eval':>16}  artifacts\")\n",
        "\n",
        "print(f\"{'baseline':>16}  {'baseline':>10}  {'n/a':>6}  {results_store['baseline']['metrics']['total']:12.6f}  {results_store['baseline']['metrics']['terminal_eval']:16.6f}  -\")\n",
        "\n",
        "for method, result in terminal_runs.items():\n",
        "    iterations = len(result.history.get('iter', []))\n",
        "    total_cost = float(result.final_metrics.get('total', float('nan')))\n",
        "    terminal_eval = terminal_eval_table[f'terminal_{method}']\n",
        "    print(f\"{('terminal_'+method):>16}  {'terminal':>10}  {iterations:6d}  {total_cost:12.6f}  {terminal_eval:16.6f}  {result.artifacts_dir}\")\n",
        "\n",
        "path_eval = terminal_eval_table['path']\n",
        "print(f\"{'path':>16}  {'path':>10}  {len(path_result.history.get('iter', [])):6d}  {path_result.final_metrics.get('total', float('nan')):12.6f}  {path_eval:16.6f}  {path_result.artifacts_dir}\")\n",
        "\n",
        "ensemble_eval = terminal_eval_table['ensemble']\n",
        "print(f\"{'ensemble':>16}  {'ensemble':>10}  {len(ensemble_result.history.get('iter', [])):6d}  {ensemble_result.final_metrics.get('total', float('nan')):12.6f}  {ensemble_eval:16.6f}  {ensemble_result.artifacts_dir}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Troubleshooting\n",
        "\n",
        "- Learning-rate/step settings per optimizer: edit `method_settings` entries if convergence is slow or unstable.\n",
        "- Armijo tuning: adjust `alpha0`, `ls_beta`, and `ls_sigma` when line search fails repeatedly.\n",
        "- Penalty weights: modify `penalties` to balance fluence/negativity contributions.\n",
        "- Basis resolution: increase `crab['K']` or customise `omegas_rad_per_us`/`phases` for more expressive pulses.\n",
        "- Ensemble grid: reduce `num_beta` / `num_detuning` for quicker sweeps or expand ranges for stress testing.\n",
        "- Path reference: ensure `path_objective` matches the trajectory you want to track.\n",
        "- Artifact storage: change `output_dir` or `run_name_prefix` to separate multiple comparison batches.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
